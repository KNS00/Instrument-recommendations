{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6\n",
    "## Implementation of the Parallel Hybrid and the LLM-Content-Based system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_table('test_new.tsv',sep='\\s+',header=0).sort_values(['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "train = pd.read_table('train_new.tsv',sep='\\s+',header=0).sort_values(['user_id', 'item_id', 'rating', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_table('metadata_cleaned.tsv', sep='\\t', header=0)\n",
    "metadata = metadata[metadata['item_id'].isin(train['item_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel combination strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('knn_user_rankings_with_ratings.json', 'r') as fp:\n",
    "    knn_user_rankings_with_rating = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('content_rankings_with_rating.json', 'r') as fp:\n",
    "    content_rankings_with_rating = json.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_test = test[test['rating'] >= 3]\n",
    "\n",
    "\n",
    "def compute_precision_at_k(top_recs, ground_truth, k):\n",
    "    top_recs = top_recs[:k]\n",
    "    ground_truth = ground_truth\n",
    "    hits = len(set(top_recs) & set(ground_truth))\n",
    "    return hits / k\n",
    "\n",
    "def compute_ap(top_recs, ground_truth):\n",
    "    ground_truth_set = set(ground_truth)    \n",
    "    G = len(ground_truth_set)\n",
    "    if G == 0:\n",
    "        return 0.0\n",
    "    cumulative_precision = 0.0\n",
    "    relevant_count = 0\n",
    "    for rank, item in enumerate(top_recs, 1):\n",
    "        if item in ground_truth_set:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / rank  # P@k\n",
    "            cumulative_precision += precision_at_k  # Sum of P@k for relevant ranks\n",
    "    ap = cumulative_precision / G\n",
    "    return ap\n",
    "def compute_metrics(user_ids, top_recommendations, k=10):\n",
    "    hit_rates = []\n",
    "    precision_scores = []\n",
    "    aps = []\n",
    "    rrs = []\n",
    "    total_items_in_catalog = len(set(train['item_id']).union(set(test['item_id'])))\n",
    "    recommended_items = set()\n",
    "    for user_id in user_ids:\n",
    "        hit = False\n",
    "        top_recs = top_recommendations.get(user_id)[:k]\n",
    "        recommended_items.update(list(top_recs))\n",
    "        ground_truth = list(relevant_test[relevant_test['user_id'] == user_id]['item_id'])\n",
    "        hits = len(set(top_recs) & set(ground_truth))\n",
    "        hit = (hits > 0).real\n",
    "        precision_at_k = compute_precision_at_k(top_recs, ground_truth, k=10)    \n",
    "        ap = compute_ap(top_recs, ground_truth)\n",
    "\n",
    "        for rank, item in enumerate(top_recs, 1):\n",
    "            if item in ground_truth:\n",
    "                rr = 1 / rank\n",
    "                break\n",
    "            else:\n",
    "                rr = 0\n",
    "        \n",
    "        coverage =  len(recommended_items) / total_items_in_catalog\n",
    "        hit_rates.append(hit)\n",
    "        precision_scores.append(precision_at_k)\n",
    "        aps.append(ap)\n",
    "        rrs.append(rr)\n",
    "    return {'PRECISION@k:': round(np.mean(precision_scores), 3), 'MAP@k:': round(np.mean(aps), 3), 'MRR@k:': round(np.mean(rrs), 3), 'Hit rate': round(np.mean(hit_rates), 3), 'Coverage': round(coverage, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRECISION@k:': 0.011,\n",
       " 'MAP@k:': 0.007,\n",
       " 'MRR@k:': 0.024,\n",
       " 'Hit rate': 0.104,\n",
       " 'Coverage': 0.577}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1_preds = knn_user_rankings_with_rating  \n",
    "model2_preds = content_rankings_with_rating   \n",
    "\n",
    "alpha = 1/3 # knn\n",
    "beta = 2/3 # content\n",
    "\n",
    "final_predictions_with_ratings = {}\n",
    "final_predictions = {}\n",
    "\n",
    "for user in set(model1_preds.keys()).union(set(model2_preds.keys())):\n",
    "    combined_preds = {}\n",
    "    \n",
    "    for item, pred in model1_preds.get(user, []):\n",
    "        combined_preds[item] = alpha * pred \n",
    "    \n",
    "    for item, pred in model2_preds.get(user, {}).items():\n",
    "        if item in combined_preds:\n",
    "            combined_preds[item] += beta * pred  \n",
    "        else:\n",
    "            combined_preds[item] = beta * pred\n",
    "    \n",
    "    sorted_items = sorted(combined_preds.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    final_predictions_with_ratings[user] = dict(sorted_items)\n",
    "    final_predictions[user] = [item for item, _ in sorted_items]\n",
    "compute_metrics(relevant_test['user_id'].unique(), final_predictions, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a4b0ebe7bbd448fb5537e2737927b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"cuda\",\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "descriptions = []  \n",
    "\n",
    "for title in metadata['title']:\n",
    "    output = pipe(\n",
    "        f\"Generate a detailed and accurate description for the following musical instrument: {title}\\n\",\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=50\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    \n",
    "    generated_text = output[0]['generated_text']\n",
    "    prompt = f\"Generate a detailed and accurate description for the following musical instrument: {title}\\n\"\n",
    "    description = generated_text.replace(prompt, \"\").strip()\n",
    "    \n",
    "    descriptions.append(description)\n",
    "\n",
    "metadata['llama-descriptions'] = descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "import pandas as pd\n",
    "with open('metadata_llama.json') as f: # saved from previous cell\n",
    "    metadata_loaded = json.load(f)\n",
    "\n",
    "metadata_loaded = json.loads(metadata_loaded)\n",
    "\n",
    "\n",
    "data = metadata_loaded['data']\n",
    "\n",
    "columns = metadata_loaded['columns']\n",
    "\n",
    "metadata_loaded = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "metadata = metadata_loaded\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = metadata[metadata['item_id'].isin(train['item_id'])] # double checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in description: 5262\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "description_before_pre_processing = metadata['llama-descriptions']\n",
    "\n",
    "vocab = set()\n",
    "for s in description_before_pre_processing:\n",
    "    #words = str.split(\" \")\n",
    "    words = word_tokenize(s, language=\"english\")\n",
    "    vocab.update(words)\n",
    "print(\"Total number of words in description:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "import string\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "metadata['llama-descriptions'] = metadata['llama-descriptions'].str.lower() # lowercasing \n",
    "metadata['llama-descriptions'] = metadata['llama-descriptions'].apply(word_tokenize, language=\"english\") #tokenizing\n",
    "english_stopwords = stopwords.words('english') + [char for char in string.punctuation]\n",
    "metadata['llama-descriptions'] = metadata['llama-descriptions'].apply(\n",
    "    lambda tokens: [token for token in tokens if token not in english_stopwords]\n",
    ") \n",
    "\n",
    "metadata['llama-stemmed'] = metadata['llama-descriptions'].apply(lambda x: [stemmer.stem(elem) for elem in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4509"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab = set()\n",
    "metadata['llama-descriptions'].apply(\n",
    "    lambda tokens: new_vocab.update(set(tokens)))\n",
    "len(new_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6q/9k_rffm16h50xgkcpmz_d3cc0000gn/T/ipykernel_61144/231285150.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  metadata['description_embedded'] = (metadata['llama-descriptions'].apply(\" \".join).apply(\n"
     ]
    }
   ],
   "source": [
    "def get_item_embedding(tokens, embeddings, embedding_size=300) -> float:\n",
    "    # Get embeddings for each word in the description\n",
    "    word_vectors = [embeddings[word] for word in tokens if word in embeddings]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(embedding_size)  # Return a zero vector of the correct size\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "metadata['description_embedded'] = (metadata['llama-descriptions'].apply(\" \".join).apply(\n",
    "    lambda tokens: get_item_embedding(tokens, word2vec_vectors)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_recommendations(user_id : str, sim_user_item_df : pd.DataFrame, k=10):\n",
    "    similarities = sim_user_item_df[user_id].sort_values(ascending=False)\n",
    "    return similarities.index[:k]\n",
    "\n",
    "def compute_precision_at_k(top_recs, ground_truth, k):\n",
    "    top_recs = top_recs[:k]\n",
    "    ground_truth = ground_truth\n",
    "    hits = len(set(top_recs) & set(ground_truth))\n",
    "    return hits / k\n",
    "\n",
    "def compute_ap(top_recs, ground_truth):\n",
    "    ground_truth_set = set(ground_truth)    \n",
    "    G = len(ground_truth_set)\n",
    "    if G == 0:\n",
    "        return 0.0\n",
    "    cumulative_precision = 0.0\n",
    "    relevant_count = 0\n",
    "    for rank, item in enumerate(top_recs, 1):\n",
    "        if item in ground_truth_set:\n",
    "            relevant_count += 1\n",
    "            precision_at_k = relevant_count / rank  # P@k\n",
    "            cumulative_precision += precision_at_k  # Sum of P@k for relevant ranks\n",
    "    ap = cumulative_precision / G\n",
    "    return ap\n",
    "def compute_metrics(user_ids, sim_user_item, k=10):\n",
    "    hit_rates = []\n",
    "    precision_scores = []\n",
    "    aps = []\n",
    "    rrs = []\n",
    "    total_items_in_catalog = len(set(train['item_id']).union(set(test['item_id'])))\n",
    "    recommended_items = set()\n",
    "    for user_id in user_ids:\n",
    "        hit = False\n",
    "        top_recs = get_top_recommendations(user_id, sim_user_item, k)\n",
    "        recommended_items.update(list(top_recs))\n",
    "        ground_truth = list(relevant_test[relevant_test['user_id'] == user_id]['item_id'])\n",
    "        hits = len(set(top_recs) & set(ground_truth))\n",
    "        hit = (hits > 0).real\n",
    "        precision_at_k = compute_precision_at_k(top_recs, ground_truth, k)    \n",
    "        ap = compute_ap(top_recs, ground_truth)\n",
    "\n",
    "        for rank, item in enumerate(top_recs, 1):\n",
    "            if item in ground_truth:\n",
    "                rr = 1 / rank\n",
    "                break\n",
    "            else:\n",
    "                rr = 0\n",
    "        \n",
    "        coverage =  len(recommended_items) / total_items_in_catalog\n",
    "        hit_rates.append(hit)\n",
    "        precision_scores.append(precision_at_k)\n",
    "        aps.append(ap)\n",
    "        rrs.append(rr)\n",
    "    return {'PRECISION@k:': round(np.mean(precision_scores), 3), 'MAP@k:': round(np.mean(aps), 3), 'MRR@k:': round(np.mean(rrs), 3), 'Hit rate': round(np.mean(hit_rates), 3), 'Coverage': round(coverage, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "feature = np.array(metadata['description_embedded'].tolist()) \n",
    "\n",
    "feature_dict = {item_id: feature_vector for item_id, feature_vector in zip(metadata['item_id'], metadata['description_embedded'])}\n",
    "\n",
    "user_profiles = pd.DataFrame(index=train['user_id'].unique())\n",
    "user_profiles['profile'] = None  \n",
    "\n",
    "user_profiles['profile'] = user_profiles['profile'].astype(object)\n",
    "\n",
    "for user_id, group in train.groupby('user_id'):\n",
    "    feature_vector = np.average([feature_dict.get(key) for key in group['item_id']], axis=0, weights=group['rating'])\n",
    "    user_profiles.at[user_id, 'profile'] = feature_vector.tolist()\n",
    "\n",
    "user_profiles_matrix = np.array(user_profiles['profile'].tolist())  \n",
    "\n",
    "llama_sim = cosine_similarity(feature, user_profiles_matrix)  \n",
    "\n",
    "llama_sim_df = pd.DataFrame(llama_sim, index=metadata['item_id'], columns=user_profiles.index)\n",
    "\n",
    "metrics = compute_metrics(relevant_test['user_id'].unique(), sim_user_item=llama_sim_df, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PRECISION@k:': 0.011,\n",
       " 'MAP@k:': 0.011,\n",
       " 'MRR@k:': 0.037,\n",
       " 'Hit rate': 0.106,\n",
       " 'Coverage': 0.403}"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B007MY5BDI'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['item_id'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The strings on this set are an improvement over any other I\\'ve used before. They give off some of that bright tone you get from lighter gauge strings but with more volume as well.\\nThese are my new favorite string! The sound is so much better than anything else out there (and yes they do come in all gauges)!! You will not be disappointed!\\nI really like these strings because they have great projection when playing at high volumes, also they last longer then most others which means less changes to your budget each time you change them. Great job by daddio!\\nI was going to say \"This sounds like something I would buy\" But it\\'s actually good enough that it can\\'t hurt anyone else either :'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['llama-descriptions'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"80/20 Bronze are our brightest acoustic strings, made to give your guitar an unparalleled shine. These bold sounding strings were developed by John D'Addario Sr. and guitar maker John D'Angelico in the 1930s, and have been beloved for their sparkling, trebly tone ever since. Made with a high carbon steel core and 80/20 Bronze wrap wire, 80/20 acoustic strings offer remarkable depth, along with rich, bright harmonics, and powerful projection. These 10-47 Extra Light gauge strings are easy to play and easy to bend.\""
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata['description'][4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WRSLabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
